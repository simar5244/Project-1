# -*- coding: utf-8 -*-
"""HCancerDental.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1et_kuvL510vFN_WD7-151lFbYI98wqk8
"""

!pip install kagglehub
!pip install tensorflow
!pip install torch torchvision torchaudio
!pip install torch-geometric
!pip install transformers
!pip install scikit-learn

# Import required libraries
import kagglehub
import os
import numpy as np
import tensorflow as tf
import torch
import torch.nn as nn
from torch_geometric.data import Data
from sklearn.model_selection import train_test_split
from transformers import ViTFeatureExtractor, ViTForImageClassification

path = kagglehub.dataset_download("ashenafifasilkebede/dataset")
print("Path to dataset files:", path)
dataset_path = path
print("Dataset directory contents:", os.listdir(dataset_path))
for subdir in os.listdir(dataset_path):
    subdir_path = os.path.join(dataset_path, subdir)
    if os.path.isdir(subdir_path):
        print(f"Contents of {subdir}: {os.listdir(subdir_path)}")
dataset_path = path
print("Dataset directory contents:", os.listdir(dataset_path))
# Function to list the number of images in each subfolder
def count_images_in_subfolders(base_path):
    image_counts = {}
    for subdir in os.listdir(base_path):
        subdir_path = os.path.join(base_path, subdir)
        if os.path.isdir(subdir_path):
            print(f"Checking folder: {subdir}")
            for category in os.listdir(subdir_path):
                category_path = os.path.join(subdir_path, category)
                if os.path.isdir(category_path):
                    image_files = [f for f in os.listdir(category_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                    image_counts[f"{subdir}/{category}"] = len(image_files)
    return image_counts

# List the number of images in each folder and category
image_counts = count_images_in_subfolders(path)

# Print the total number of images for each folder/category
for folder_category, count in image_counts.items():
    print(f"Total images in {folder_category}: {count}")

import random
import matplotlib.pyplot as plt
import tensorflow as tf

# List total images
def list_and_show_random_image(base_path):
    for subdir in os.listdir(base_path):
        subdir_path = os.path.join(base_path, subdir)
        if os.path.isdir(subdir_path):
            print(f"Listing images in {subdir_path}...")
            image_files = [f for f in os.listdir(subdir_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            print(f"Total images in {subdir}: {len(image_files)}")
            if len(image_files) > 0:
                # Pick a random image
                random_img = random.choice(image_files)
                img_path = os.path.join(subdir_path, random_img)
                img = tf.io.read_file(img_path)
                img = tf.image.decode_jpeg(img, channels=3)
                img = tf.image.resize(img, (224, 224))
                img = img / 255.0  # Normalize
                plt.imshow(img)
                plt.title(f"Random image from {subdir}")
                plt.show()

# List images in the 'train' folder and show one random image
list_and_show_random_image(os.path.join(path, 'train'))

# List the contents of the 'train' directory to ensure correct folder structure
train_dir = os.path.join(path, 'train')
print(f"Contents of train directory: {os.listdir(train_dir)}")

# List contents of 'OSCC' and 'Normal' directories
oscc_dir = os.path.join(train_dir, 'OSCC')
normal_dir = os.path.join(train_dir, 'Normal')

def load_images_and_labels(base_path, img_size=(224, 224)):
    images = []
    labels = []

    # Ensure the directories are correct
    oscc_dir = os.path.join(base_path, 'OSCC')
    normal_dir = os.path.join(base_path, 'Normal')

    # Load images and labels from the 'OSCC' folder
    for img_file in os.listdir(oscc_dir):
        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
            img_path = os.path.join(oscc_dir, img_file)
            img = tf.io.read_file(img_path)
            img = tf.image.decode_jpeg(img, channels=3)
            img = tf.image.resize(img, img_size)
            img = img / 255.0  # Normalize
            images.append(img)
            labels.append(1)  # OSCC label is 1

    # Load images and labels from the 'Normal' folder
    for img_file in os.listdir(normal_dir):
        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
            img_path = os.path.join(normal_dir, img_file)
            img = tf.io.read_file(img_path)
            img = tf.image.decode_jpeg(img, channels=3)
            img = tf.image.resize(img, img_size)
            img = img / 255.0  # Normalize
            images.append(img)
            labels.append(0)  # Normal label is 0

    return np.array(images), np.array(labels)

# Load the images and labels
images, labels = load_images_and_labels(train_dir)

"""Algorithm 1"""

import random
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Set dataset path
dataset_path = '/root/.cache/kagglehub/datasets/ashenafifasilkebede/dataset/versions/1'

# Data augmentation and normalization
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

# Prepare training data (80%)
train_generator = datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'),
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='training')

# Prepare validation data (20%)
val_generator = datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'),
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='validation')

# Build Feedforward Neural Network model
model = models.Sequential([
    layers.Flatten(input_shape=(224, 224, 3)),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(train_generator, epochs=10, validation_data=val_generator)

# Save the model
model.save('oscc_model.h5')

# Evaluate model
loss, accuracy = model.evaluate(val_generator)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.metrics import ConfusionMatrixDisplay
from tensorflow.keras.models import load_model  # Import the load_model function

# Load the trained model
model = load_model('oscc_model.h5')

# Get the true labels and predicted probabilities
y_true = val_generator.classes  # True labels from the validation generator
y_pred_prob = model.predict(val_generator)  # Predicted probabilities

# Convert probabilities to binary predictions (threshold 0.5)
y_pred = (y_pred_prob > 0.5).astype(int)

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_generator.class_indices)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

# ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt

# Load the trained model
model = load_model('oscc_model.h5')

# Get the true labels and predicted probabilities
y_true = val_generator.classes  # True labels from the validation generator
y_pred_prob = model.predict(val_generator)  # Predicted probabilities

# Convert probabilities to binary predictions (threshold 0.5)
y_pred = (y_pred_prob > 0.5).astype(int)

"""Algorithm 2"""

import tensorflow as tf
import os
import numpy as np
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16

# Set dataset path
dataset_path = '/root/.cache/kagglehub/datasets/ashenafifasilkebede/dataset/versions/1'

# Data augmentation and normalization
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

# Prepare training data (80%)
train_generator = datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'),
    target_size=(128, 128),  # Reduce image size to speed up training
    batch_size=8,  # Decrease batch size to fit in memory
    class_mode='binary',
    subset='training')

# Prepare validation data (20%)
val_generator = datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'),
    target_size=(128, 128),
    batch_size=8,
    class_mode='binary',
    subset='validation')

# Build VGG16 model as an alternative to CNN and Vision Transformer
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
base_model.trainable = False  # Freeze the base model layers

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),  # Use GAP instead of Flatten to reduce parameters
    layers.Dense(64, activation='relu'),  # Reduce dense layer size
    layers.Dropout(0.3),  # Reduce dropout to maintain accuracy
    layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary
model.summary()

# Train the model with fewer epochs
history = model.fit(train_generator, epochs=10, validation_data=val_generator)

# Save the model
model.save('oscc_model_vgg16.h5')

# Evaluate model
loss, accuracy = model.evaluate(val_generator)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, precision_recall_curve
import seaborn as sns
from sklearn.metrics import ConfusionMatrixDisplay

# Get model predictions
y_true = val_generator.classes  # True labels
y_pred = model.predict(val_generator)  # Predicted probabilities

# Convert probabilities to binary class predictions
y_pred_binary = (y_pred > 0.5).astype(int)

# Generate confusion matrix
cm = confusion_matrix(y_true, y_pred_binary)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_generator.class_indices)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Generate Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_true, y_pred)

# Plot Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(True)
plt.show()

# Optionally, calculate and display F1 Score
from sklearn.metrics import f1_score
f1 = f1_score(y_true, y_pred_binary)
print(f"F1 Score: {f1:.2f}")

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Get model predictions
y_true = val_generator.classes  # True labels
y_pred = model.predict(val_generator)  # Predicted probabilities

# Compute the ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_true, y_pred)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""Algorithm Number 3 (Completed)

"""

import tensorflow as tf
import os
import numpy as np
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0

# Set dataset path
dataset_path = '/root/.cache/kagglehub/datasets/ashenafifasilkebede/dataset/versions/1'

# Data augmentation and normalization
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

# Prepare training data (80%)
train_generator = datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'),
    target_size=(128, 128),
    batch_size=16,  # Increased batch size for better generalization
    class_mode='binary',
    subset='training')

# Prepare validation data (20%)
val_generator = datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'),
    target_size=(128, 128),
    batch_size=16,
    class_mode='binary',
    subset='validation')

# Build EfficientNetB0 model as an alternative to VGG16
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
base_model.trainable = False  # Freeze the base model layers

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),  # Increased dense layer for better learning
    layers.Dropout(0.4),  # Increased dropout to reduce overfitting
    layers.Dense(1, activation='sigmoid')
])

# Compile the model with a lower learning rate
model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

# Summary
model.summary()

# Train the model with fewer epochs
history = model.fit(train_generator, epochs=10, validation_data=val_generator)

# Save the model
model.save('oscc_model_efficientnet.h5')

# Evaluate model
loss, accuracy = model.evaluate(val_generator)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred_binary)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve & AUC Score
fpr, tpr, _ = roc_curve(y_true, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

"""Algorithm 4 (Completed)"""

import os
import random
import numpy as np
import cv2
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

# Set dataset path (replace with the correct path)
dataset_path = '/root/.cache/kagglehub/datasets/ashenafifasilkebede/dataset/versions/1'

# Function to extract SIFT features from an image
def extract_sift_features(image_path):
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Initialize SIFT detector
    sift = cv2.SIFT_create()

    # Detect keypoints and descriptors
    kp, des = sift.detectAndCompute(gray, None)

    if des is not None:
        return np.mean(des, axis=0)  # Using mean of descriptors as a feature vector
    else:
        return np.zeros(128)  # If no descriptors found, return zero vector

# Function to preprocess images and extract features (use only 20% of data)
def preprocess_images(base_path, sample_fraction=0.2):
    image_data = []
    labels = []

    for subdir in os.listdir(base_path):
        subdir_path = os.path.join(base_path, subdir)
        if os.path.isdir(subdir_path):
            for category in os.listdir(subdir_path):
                category_path = os.path.join(subdir_path, category)
                if os.path.isdir(category_path):
                    image_files = [f for f in os.listdir(category_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                    random.shuffle(image_files)
                    image_files = image_files[:int(len(image_files) * sample_fraction)]  # Use only 20% of data

                    for image_file in image_files:
                        image_path = os.path.join(category_path, image_file)
                        features = extract_sift_features(image_path)
                        image_data.append(features)
                        labels.append(category)

    # Convert labels to numeric values
    label_encoder = LabelEncoder()
    labels = label_encoder.fit_transform(labels)

    return np.array(image_data), np.array(labels)

# Preprocess images and extract features from 20% of the data
image_data, labels = preprocess_images(dataset_path, sample_fraction=0.2)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(image_data, labels, test_size=0.2, random_state=42)

# Initialize and train Random Forest Classifier (using a smaller max_depth for efficiency)
rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

import seaborn as sns
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using seaborn for better visualization
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(labels), yticklabels=np.unique(labels))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt

# Binarize the true labels for ROC AUC curve computation (one-vs-rest)
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
n_classes = y_test_bin.shape[1]

# Get predicted probabilities for each class
y_pred_prob = rf_classifier.predict_proba(X_test)

# Initialize dictionaries for FPR, TPR, and AUC
fpr, tpr, roc_auc = {}, {}, {}

# Compute ROC curve and AUC for each class
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curve
plt.figure(figsize=(8, 6))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], lw=2, label=f'ROC curve of class {i} (area = {roc_auc[i]:0.2f})')

# Plot the random guessing line (diagonal)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')

# Formatting the plot
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()